{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://127.0.0.1:5000/v1/ \n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from retriever_module.retriever import llm_wrapper\n",
    "app = Flask(__name__)\n",
    "\n",
    "# # 加载模型和分词器\n",
    "# llm = llm_wrapper(model_path_or_name=\"/home/simon/disk1/Simon/Code/LLM/models/meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# llm(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "import torch \n",
    "class MultiConversation: \n",
    "    def __init__(self, model_name, init_prompt, device='cuda' if torch.cuda.is_available() else 'cpu'): \n",
    "        nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.device = device \n",
    "        self.tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config = nf4_config)\n",
    "        self.messages = [] \n",
    "        self.messages.append(self.tokenizer(init_prompt, return_tensors=\"pt\"))\n",
    "    def qa(self, question): \n",
    "        # 将新问题添加到消息列表中 \n",
    "        question_tensors = self.tokenizer(question, return_tensors=\"pt\")\n",
    "        # 将新问题和历史消息合并（注意：这里需要稍微处理一下，因为模型可能需要特定的输入格式） \n",
    "        # # 假设模型可以直接接受这样的序列（这取决于模型的实际要求） \n",
    "        # # 注意：这里只是简单地将它们连接在一起，实际情况可能需要更复杂的处理 \n",
    "        # print(self.messages)\n",
    "        # print([m['input_ids'] for m in self.messages])\n",
    "        print([m['input_ids'] for m in self.messages] + [question_tensors['input_ids']])\n",
    "        combined_input = torch.cat([m['input_ids'] for m in self.messages] + [question_tensors['input_ids']], dim=1) \n",
    "        attention_mask = torch.cat([torch.ones_like(m['attention_mask']) for m in self.messages] + [question_tensors['attention_mask']], dim=1) \n",
    "        # 你可以根据需要调整这些参数（如 max_length, top_p, temperature 等） \n",
    "        generated_ids = self.model.generate( input_ids=combined_input, attention_mask=attention_mask, max_length=100, \n",
    "                                            # 你可以根据需要调整这个长度 \n",
    "                                            temperature=1.0, top_p=0.9, top_k=0, repetition_penalty=1.0, do_sample=True, ) \n",
    "        # 解码生成的 ID 以获取文本 \n",
    "        answer = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True) \n",
    "        #保存回答到消息列表中（这里只是简单记录，实际使用可能需要更复杂的数据结构\n",
    "        self.messages.append({ 'input_ids': question_tensors['input_ids'], 'attention_mask': question_tensors['attention_mask'], 'generated_text': answer }) \n",
    "        return answer \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/simon/miniconda3/envs/LLM/lib/python3.10/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[9906,   11, 1268,  527,  499,   30]]), tensor([[  40, 2846, 7060,   13, 2650,  922,  499,   30]])]\n",
      "Hello, how are you?I'm fine. How about you?I'm good, thanks for asking. How's the new job?It's going well. I like it a lot.\n"
     ]
    }
   ],
   "source": [
    "#使用示例 \n",
    "model_name = '/home/simon/disk1/Simon/Code/LLM/models/nvidia/Llama3-ChatQA-1.5-8B' \n",
    "init_prompt = \"Hello, how are you?\" \n",
    "conv = MultiConversation(model_name, init_prompt) \n",
    "print(conv.qa(\"I'm fine. How about you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 实现一个多轮问答的类\n",
    "# class MultiConversation:\n",
    "#     # 初始化对象时传入第一句指令\n",
    "#     def __init__(self, init_prompt):\n",
    "#         self.messages = []\n",
    "#         self.messages.append({\"role\": \"system\", \"content\": init_prompt})\n",
    "\n",
    "#     # 每调用一次这个方法，都会将问题和回答记录在self.messages列表属性中，用于记录上下文\n",
    "#     def qa(self, question):\n",
    "#         self.messages.append({\"role\": \"user\", \"content\": question})\n",
    "#         answer = \n",
    "#         self.messages.append({\"role\": \"assistant\", \"content\": answer.choices[0].message.content})\n",
    "#         return answer.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = '/home/simon/disk1/Simon/Code/LLM/models/nvidia/Llama3-ChatQA-1.5-8B' \n",
    "init_prompt = \"you are a userful helper\" \n",
    "conv = MultiConversation(model_name, init_prompt) \n",
    "@app.route('/v1/chat/completions', methods=['POST'])\n",
    "def predict():\n",
    "    # 解析请求数据\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    \n",
    "    ans = conv.qa(text)\n",
    "    return jsonify({'content': ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 3256561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（并非所有进程都能被检测到，所有非本用户的进程信息将不会显示，如果想看到所有信息，则必须切换到 root 用户）\n",
      "激活Internet连接 (仅服务器)\n",
      "Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \n",
      "tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 0.0.0.0:1200            0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 0.0.0.0:5455            0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 0.0.0.0:9000            0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 0.0.0.0:9001            0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 0.0.0.0:9380            0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 127.0.0.1:34183         0.0.0.0:*               LISTEN      470266/python       \n",
      "tcp        0      0 127.0.0.1:34105         0.0.0.0:*               LISTEN      1141520/python      \n",
      "tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 127.0.0.1:33331         0.0.0.0:*               LISTEN      3257846/clash-verge \n",
      "tcp        0      0 127.0.0.1:36609         0.0.0.0:*               LISTEN      2036861/python      \n",
      "tcp        0      0 127.0.0.1:34915         0.0.0.0:*               LISTEN      3978608/python      \n",
      "tcp        0      0 127.0.0.1:35600         0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 127.0.0.1:38047         0.0.0.0:*               LISTEN      2028748/python      \n",
      "tcp        0      0 127.0.0.1:5939          0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 127.0.0.1:7897          0.0.0.0:*               LISTEN      3257966/clash-meta  \n",
      "tcp        0      0 127.0.0.1:7898          0.0.0.0:*               LISTEN      3257966/clash-meta  \n",
      "tcp        0      0 127.0.0.1:7895          0.0.0.0:*               LISTEN      3257966/clash-meta  \n",
      "tcp        0      0 127.0.0.1:7890          0.0.0.0:*               LISTEN      3257966/clash-meta  \n",
      "tcp        0      0 127.0.0.1:42623         0.0.0.0:*               LISTEN      3256561/python      \n",
      "tcp        0      0 127.0.0.1:9097          0.0.0.0:*               LISTEN      3257966/clash-meta  \n",
      "tcp        0      0 127.0.0.1:9048          0.0.0.0:*               LISTEN      470266/python       \n",
      "tcp        0      0 127.0.0.1:9049          0.0.0.0:*               LISTEN      470266/python       \n",
      "tcp        0      0 127.0.0.1:9050          0.0.0.0:*               LISTEN      470266/python       \n",
      "tcp        0      0 127.0.0.1:9051          0.0.0.0:*               LISTEN      470266/python       \n",
      "tcp        0      0 127.0.0.1:9044          0.0.0.0:*               LISTEN      3256561/python      \n",
      "tcp        0      0 127.0.0.1:9045          0.0.0.0:*               LISTEN      3256561/python      \n",
      "tcp        0      0 127.0.0.1:9046          0.0.0.0:*               LISTEN      3256561/python      \n",
      "tcp        0      0 127.0.0.1:9047          0.0.0.0:*               LISTEN      470266/python       \n",
      "tcp        0      0 127.0.0.1:9042          0.0.0.0:*               LISTEN      3256561/python      \n",
      "tcp        0      0 127.0.0.1:9043          0.0.0.0:*               LISTEN      3256561/python      \n",
      "tcp        0      0 127.0.0.1:9036          0.0.0.0:*               LISTEN      1141520/python      \n",
      "tcp        0      0 127.0.0.1:9037          0.0.0.0:*               LISTEN      1141520/python      \n",
      "tcp        0      0 127.0.0.1:9038          0.0.0.0:*               LISTEN      1141520/python      \n",
      "tcp        0      0 127.0.0.1:9039          0.0.0.0:*               LISTEN      1141520/python      \n",
      "tcp        0      0 127.0.0.1:9032          0.0.0.0:*               LISTEN      3978608/python      \n",
      "tcp        0      0 127.0.0.1:9033          0.0.0.0:*               LISTEN      3978608/python      \n",
      "tcp        0      0 127.0.0.1:9034          0.0.0.0:*               LISTEN      3978608/python      \n",
      "tcp        0      0 127.0.0.1:9035          0.0.0.0:*               LISTEN      1141520/python      \n",
      "tcp        0      0 127.0.0.1:9030          0.0.0.0:*               LISTEN      3978608/python      \n",
      "tcp        0      0 127.0.0.1:9031          0.0.0.0:*               LISTEN      3978608/python      \n",
      "tcp        0      0 127.0.0.1:9024          0.0.0.0:*               LISTEN      2957687/python      \n",
      "tcp        0      0 127.0.0.1:9060          0.0.0.0:*               LISTEN      1709195/python      \n",
      "tcp        0      0 127.0.0.1:9061          0.0.0.0:*               LISTEN      1709195/python      \n",
      "tcp        0      0 127.0.0.1:9057          0.0.0.0:*               LISTEN      1709195/python      \n",
      "tcp        0      0 127.0.0.1:9058          0.0.0.0:*               LISTEN      1709195/python      \n",
      "tcp        0      0 127.0.0.1:9059          0.0.0.0:*               LISTEN      1709195/python      \n",
      "tcp        0      0 127.0.0.1:9020          0.0.0.0:*               LISTEN      2957687/python      \n",
      "tcp        0      0 127.0.0.1:9021          0.0.0.0:*               LISTEN      2957687/python      \n",
      "tcp        0      0 127.0.0.1:9022          0.0.0.0:*               LISTEN      2957687/python      \n",
      "tcp        0      0 127.0.0.1:9023          0.0.0.0:*               LISTEN      2957687/python      \n",
      "tcp        0      0 127.0.0.1:9016          0.0.0.0:*               LISTEN      2449395/python      \n",
      "tcp        0      0 127.0.0.1:9017          0.0.0.0:*               LISTEN      2449395/python      \n",
      "tcp        0      0 127.0.0.1:9018          0.0.0.0:*               LISTEN      2449395/python      \n",
      "tcp        0      0 127.0.0.1:9019          0.0.0.0:*               LISTEN      2449395/python      \n",
      "tcp        0      0 127.0.0.1:9012          0.0.0.0:*               LISTEN      2028748/python      \n",
      "tcp        0      0 127.0.0.1:9013          0.0.0.0:*               LISTEN      2028748/python      \n",
      "tcp        0      0 127.0.0.1:9014          0.0.0.0:*               LISTEN      2028748/python      \n",
      "tcp        0      0 127.0.0.1:9015          0.0.0.0:*               LISTEN      2449395/python      \n",
      "tcp        0      0 127.0.0.1:9008          0.0.0.0:*               LISTEN      2036861/python      \n",
      "tcp        0      0 127.0.0.1:9009          0.0.0.0:*               LISTEN      2036861/python      \n",
      "tcp        0      0 127.0.0.1:9010          0.0.0.0:*               LISTEN      2028748/python      \n",
      "tcp        0      0 127.0.0.1:9011          0.0.0.0:*               LISTEN      2028748/python      \n",
      "tcp        0      0 127.0.0.1:9005          0.0.0.0:*               LISTEN      2036861/python      \n",
      "tcp        0      0 127.0.0.1:9006          0.0.0.0:*               LISTEN      2036861/python      \n",
      "tcp        0      0 127.0.0.1:9007          0.0.0.0:*               LISTEN      2036861/python      \n",
      "tcp        0      0 127.0.0.1:11434         0.0.0.0:*               LISTEN      -                   \n",
      "tcp        0      0 127.0.0.1:46489         0.0.0.0:*               LISTEN      1709195/python      \n",
      "tcp        0      0 127.0.0.1:45417         0.0.0.0:*               LISTEN      2957687/python      \n",
      "tcp        0      0 127.0.0.1:45303         0.0.0.0:*               LISTEN      2449395/python      \n",
      "tcp6       0      0 :::80                   :::*                    LISTEN      -                   \n",
      "tcp6       0      0 :::443                  :::*                    LISTEN      -                   \n",
      "tcp6       0      0 :::1200                 :::*                    LISTEN      -                   \n",
      "tcp6       0      0 :::5455                 :::*                    LISTEN      -                   \n",
      "tcp6       0      0 :::9000                 :::*                    LISTEN      -                   \n",
      "tcp6       0      0 :::9001                 :::*                    LISTEN      -                   \n",
      "tcp6       0      0 :::9380                 :::*                    LISTEN      -                   \n",
      "tcp6       0      0 ::1:631                 :::*                    LISTEN      -                   \n"
     ]
    }
   ],
   "source": [
    "!netstat -nplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND     PID  USER   FD   TYPE    DEVICE SIZE/OFF NODE NAME\n",
      "python  1709195 simon   10u  IPv4 183131328      0t0  TCP localhost:9059 (LISTEN)\n",
      "python  1709195 simon   17u  IPv4 182690398      0t0  TCP localhost:32808->localhost:9059 (ESTABLISHED)\n",
      "python  1709195 simon   18u  IPv4 182690399      0t0  TCP localhost:9059->localhost:32808 (ESTABLISHED)\n",
      "python  1709195 simon   60u  IPv4 183131346      0t0  TCP localhost:9059->localhost:32822 (ESTABLISHED)\n",
      "code    3255260 simon   81u  IPv4 182690403      0t0  TCP localhost:32822->localhost:9059 (ESTABLISHED)\n"
     ]
    }
   ],
   "source": [
    "!lsof -i:9059"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
